---
title: "Remotely Sensing Cities and Environments"
#subtitle: "⚔<br/>with xaringan"
author: "Andy MacLachlan"
output:
  xaringan::moon_reader:
    css: ["custom.css", "default", "rladies", "rladies-fonts"]    
    lib_dir: libs
    includes:
      in_header: [assets/header.html]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    seal: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-all, echo=FALSE}
library(countdown)
library(xaringan)
library(xaringanExtra)
library(knitr)

hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  x <- stringr::str_replace(x, "^[[:blank:]]?([^*].+?)[[:blank:]]*#<<[[:blank:]]*$", "*\\1")
  hook_source(x, options)
})

xaringanExtra::use_tachyons()
xaringanExtra::use_broadcast()
xaringanExtra::use_freezeframe()
xaringanExtra::use_scribble()
#xaringanExtra::use_slide_tone()
xaringanExtra::use_search(show_icon = TRUE, auto_search = FALSE)
xaringanExtra::use_freezeframe()
xaringanExtra::use_clipboard()
xaringanExtra::use_tile_view()
xaringanExtra::use_panelset()
xaringanExtra::use_editable(expires = 1)
xaringanExtra::use_fit_screen()
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         
  mute_unhighlighted_code = TRUE  
)

```

class: center, title-slide, middle

background-image: url("img/CASA_Logo_no_text_trans_17.png")
background-size: cover
background-position: center


<style>
.title-slide .remark-slide-number {
  display: none;
}
</style>


```{r load_packages, message=FALSE, warning=FALSE, include=FALSE}
library(fontawesome)
```

# Remotely Sensing Cities and Environments

### Lecture 9: Synthetic Aperture Radar (SAR) data

### 4/10/2022 (updated: `r format(Sys.time(), "%d/%m/%Y")`)

`r fa("paper-plane", fill = "#562457")`[a.maclachlan@ucl.ac.uk](mailto:a.maclachlan@ucl.ac.uk)
`r fa("twitter", fill = "#562457")`[andymaclachlan](https://twitter.com/andymaclachlan)
`r fa("github", fill = "#562457")`[andrewmaclachlan](https://github.com/andrewmaclachlan)
`r fa("location-dot", fill = "#562457")`[Centre for Advanced Spatial Analysis, UCL](https://www.ucl.ac.uk/bartlett/casa/)

<a href="https://github.com/andrewmaclachlan" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#fff; color:#151513; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

---

```{r, echo=FALSE}
xaringanExtra::use_progress_bar(color = "#0051BA", location = "bottom")
```

# How to use the lectures


- Slides are made with [xaringan](https://slides.yihui.org/xaringan/#1)

- `r fa("magnifying-glass")` In the bottom left there is a search tool which will search all content of presentation

- Control + F will also search 

- Press enter to move to the next result 

- `r fa("pencil")` In the top right let's you draw on the slides, although these aren't saved.

- Pressing the letter `o` (for overview) will allow you to see an overview of the whole presentation and go to a slide

- Alternatively just typing the slide number e.g. 10 on the website will take you to that slide

- Pressing alt+F will fit the slide to the screen, this is useful if you have resized the window and have another open - side by side. 

```{r xaringan-logo, echo=FALSE}
xaringanExtra::use_logo(
  image_url = "img/casa_logo.jpg",
  width = "50px",
  position = xaringanExtra::css_position(top = "1em", right = "2em")
)
```
---
# Lecture outline

.pull-left[

### Part 1: SAR fundamentals 


### Part 2: Practical change detection with SAR 

]

--
.pull-right[
```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('img/satellite.png')
```
.small[Source:[Original from the British Library. Digitally enhanced by rawpixel.](https://www.rawpixel.com/image/571789/solar-generator-vintage-style)
]]

---
class: inverse, center, middle

# Let's recall some of the intro slides....

---

# The two types of sensor

.pull-left[
```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('img/Passive-and-active-sensors-systems-working-principles-24.png')
```
.small[Source: https://www.researchgate.net/figure/Passive-and-active-sensors-systems-working-principles-24_fig2_344464269]
]

.pull-right[
### Active

* Have an energy source for illumination 
* Actively emits electormagentic waves and then waits to receive 
* Such as: Radar, X-ray, LiDAR

### Passive  

* Use energy that is available
* Don't emit anything 
* Usually detecting **reflected** energy from the sun
* Energy is in electromagnetic waves...
* Such as: Human eye, camera, satellite sensor

Sensors can be mounted on any platform. 
]


---
class: inverse, center, middle

# Now some of the lecture 4 slides...and a few extras

---

# SAR floods

**Sensor**

* Sentinel-1 SAR 

ENSO phases but this is from Australian La Niña 2022
  * trade winds from south america intensity
  * draw up cool deep waters and increase thermocline 
  * temp difference increases, walker circulation intensifies - feedback loop
  * more cloud + more rain + cyclones in West Pacific 

```{r echo=FALSE, out.width='35%', fig.align='center'}
knitr::include_graphics('img/SAR1.jpg')
```

.small[Eastern Australia Floods. Source:[brockmann-consult](https://www.brockmann-consult.de/eastern-australia-floods/)
]


---

# SAR background

Synthetic Aperture Radar:
  * Active sensors
  * Have surface texture data
  * See through weather and clouds 
  * Different wavelengths - different applications
  
```{r echo=FALSE, out.width='50%', fig.align='center'}
knitr::include_graphics('img/SAR_bands.png')
```

.small[What is Synthetic Aperture Radar?. Source:[NASA Earth Data](https://earthdata.nasa.gov/learn/backgrounders/what-is-sar)
]


---
# SAR background
.pull-left[

1. Emit an electromagnetic signal (speed of light)
1. Record the amount of signal that bounces back = "backscatter"

But...the Radar is moving...

1. Moves forward (in the azimuth) - longer antenna has a narrower beam and high resolution
1. Sweeps the footprint (swath)

Images...

1. Pixels in swath are imaged many times
1. **means** the distance will change and we know exactly how much by phase...
1. We combine these images to make **"synthetic" aperture**
]

.pull-right[

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/Radar_in_motion_diagram.jpeg')
```

.small[Get to know SAR. Source: [NASA](https://nisar.jpl.nasa.gov/mission/get-to-know-sar/overview/)
]
]

---
#SAR terms

.pull-left[
* Photography **aperture** = lets more light in to change focus
```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("img/aperture-rules.jpg")
```
.small[What is Aperture? Source:[City Academy](https://www.city-academy.com/news/what-is-aperture-in-photography/)
]
]
.pull-right[

* RADAR aperture = the antenna 
```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("img/seasat_landing_image-300x300.png")
```
.small[What is Synthetic Aperture Radar? Source:[NASA Earth Data](https://earthdata.nasa.gov/learn/backgrounders/what-is-sar)
]
]

---

class: inverse, center, middle

## Longer antenna = narrower beam and a higher resolution


## BUT we can't have a long antenna in space..

## "synthetic" aperture = "synthesize a long antenna by combining signals, or echoes, received by the radar as it moves along a flight track"


.small[[Source: NASA get to know SAR](https://nisar.jpl.nasa.gov/mission/get-to-know-sar/overview/)
]

---

# SAR polarization

.pull-left[

  * Also different ploarizations:
    * orientation of the plane in which EMR waves transmitted..
    * "direction of travel of an electromagnetic wave vector’s tip: vertical (up and down), horizontal (left to right), or circular (rotating in a constant plane left or right)."
    
* Single = 1 horizontal (or vertical)  

* Dual = transmits and receives both horizontal and vertical

* HH = emitted in horizontal (H) and received in horizontal (H)
    
]

.pull-right[

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/polarisation.png')
```

.small[Polarization. Source:[Wetland Monitoring and Mapping Using Synthetic Aperture Radar](https://www.intechopen.com/chapters/63701)
]

]
 
---
  
# SAR polarization

Different surfaces respond differently to the polarizations

* Rough scattering (e.g. bare earth) = most sensitive to VV
* Volume scattering (e.g. leaves) = cross, VH or HV
* Double bounce (e.g. trees / buildings) = most sensitive to HH.

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/SARPolarization.jpg')
```

.small[What is Synthetic Aperture Radar?. Source:[NASA Earth Data](https://earthdata.nasa.gov/learn/backgrounders/what-is-sar)
]

---

# SAR background 

Scattering can change based on wavelength 

Further penetration then the volume scattering will change

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/SARtree_figure2.jpg')
```

.small[What is Synthetic Aperture Radar?. Source:[NASA Earth Data](https://earthdata.nasa.gov/learn/backgrounders/what-is-sar)
]


---

# SAR background 

.pull-left[

* Wavelength of SAR can change application 

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/SAR_bands_NASA.png')
```
]

.pull-right[

* Remember this is on the electromagnetic spectrum (EMR)

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/SAR_bands_EMR.jpg')
```
]
.small[What is Synthetic Aperture Radar?. Source:[NASA Earth Data](https://earthdata.nasa.gov/learn/backgrounders/what-is-sar)
]

---

# Amplitude (backscatter) and phase


.pull-left[
A SAR signal has both **amplitude** (backscatter) and **phase** data

** Backscatter (amplitude)**

* Polarization

  * VV = surface roughness
  * VH = volume of surface (e.g. vegetation has a complex volume and can change the polarization)

* Permativity (dielectric constant) - how **reflective** is the property which means **reflective back to the sensor**. Water usually reflects it off elsewhere

* The return **value**, also remember the band (wavelength)
]

.pull-right[

```{r echo=FALSE, out.width='100%', fig.align='center'}
  knitr::include_graphics('img/amplitude_example.png')
```
* Wind makes the water move and reflect back to the sensor (under VV)

.small[NASA Data Made Easy: Part 2- Introduction to SAR. Source:[NASA Earth Data](https://www.youtube.com/watch?v=Zfn7P395O40)
]
]

---

# Amplitude (backscatter) and phase

.pull-left[
A SAR signal has both **amplitude** (backscatter) and **phase** data
  
**Phase**

  * Location of wave on the cycle when it comes back to the sensor

]

.pull-right[

```{r echo=FALSE, out.width='100%', fig.align='center'}
  knitr::include_graphics('img/phas_shift.png')
```

.small[InSAR. Source:[Pascal Castellazzi](https://www.researchgate.net/figure/Principle-of-the-InSAR-techniques-the-phase-difference-observed-by-comparing-two-SAR_fig1_342916517)
]
]

---
# InSAR

InSAR is mapping for ground movement detected through this **phase shift**

Movement is shown through an **interferogram**

```{r echo=FALSE, out.width='70%', fig.align='center'}
  knitr::include_graphics('img/phase_shift.jpg')
```

.small[InSAR. Source:[GeoScience Australia](https://www.ga.gov.au/scientific-topics/positioning-navigation/geodesy/geodetic-techniques/interferometric-synthetic-aperture-radar)
]

---

# Differential Interferometric Synthetic Aperture Radar (DInSAR)

The field of this work is called **interferometry**

Phase shift might come from topography (hills etc) if this is the case (it might not be) then we can remove the effect and this is termed **Differential Interferometry (DInSAR)** 

Typical workflow will include considering the elevation and 


---

# InSAR or DInSAR

* SAR = active sensor, see through clouds, records energy reflected back 

* InSAR = used for DEMs, converting phase different to relative height

* DInSAR = changes between two images in time. Looking at movement of land (uplift or sinking) with topography removed (using a DEM)

---

# SAR data processing


.pull-left[
Each pixel is a real/imaginary number know as I and  Q, meaning an in-phase(I) and quadrature pair (Q), this comes from ** electrical engineering**

To create a visible image (amplitude) it is **detected** = the Ground Range Detection produce in GEE:

  * square root of the sum of squared from the I and Q values in the Single Look Complex (SLC) product
  * This makes intensity (backscatter) 

Values can be stored in power, amplitude or dB....
]

.pull-right[
```{r echo=FALSE, out.width='100%', fig.align='center'}
  knitr::include_graphics('img/product_tree.png')
```

.small[The Tree of Processing Options. Source:[ICEYE](https://iceye-ltd.github.io/product-documentation/5.0/productFormats/introduction/)
]

]


---

class: inverse, center, middle

## In GEE

## Only **amplitude (backscatter)** data is available..

## To use phase data we need to use SNAP (not considered here) 

---

# GEE SAR data

* Notice that it is logged - why?
  * Show the full range of values
* To get non logged data - why?
  * Undertake calculations 

```{r, eval=FALSE}
ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT')
```

```{r echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics('img/SAR_GEE.png')
```

---

# SAR data values

Sentinel-1 Radiometric Terrain Corrected (RTC) data is typically provided on a **power** scale...

** power scale **, **RAW data**

* These values are low and close to 0
  * means that bright areas are skewed (you can't see any difference)
  * good for statistics / analysis, poor for visualisation 

** amplitude scale **

* square root of the power scale values
  * brighter darker pixels and darken bright pixels - narrows range
  * good for visualisation + log changes (see later slides)

** dB scale **, **in GEE**

* multiplying 10 times the Log10 of the power scale values
  * good for identifying differences in dark pixels (e.g. water)
  * not great for visualistion can be "washed out"
  * not useful for statistical analysis (log scale)

.small[Introduction to SAR. Source:[Alaska Satellite Facility](https://hyp3-docs.asf.alaska.edu/guides/introduction_to_sar/#introduction-to-sar)
]


---

# Examples 

See [the scale conversion tool by Heidi Kristenson, ASF](https://storymaps.arcgis.com/stories/73b6af082e1f44bca8a0c5fb6bf09f37)

The [power of SAR seeing through clouds](https://storymaps.arcgis.com/stories/2ead3222d2294d1fae1d11d3f98d7c35)

---

# GEE SAR data...a closer look

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/SAR_GEE.png')
```

---

# Questions to help us decide

We need to consider...

**What are we trying to detect, roughness or volume of a material...**

* Polarization

  * VV = surface roughness
  * VH = volume of surface (e.g. vegetation has a complex volume and can change the polarization)

**What are we trying to do...**

* Type of data

  * power scale = analysis
  * amplitude scale = visualisation 
  * dB scale = dark pixel differences

**what part of earth are we looking at...**

  * lot's of water?

---

class: inverse, center, middle

## How to we identify change then

--

## At the moment there seems to be no consistent approach to this question...

---

# Identifying change...

As we now know enough about SAR data...how do we establish changes between time?

** subtract images **
* Sometimes with optical data we can subtract images to determine differences

  * This is not a good idea with SAR data

> So difference pixels in bright areas will have a higher variance than difference pixels in darker areas [(Mort Canty)](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-2)

> image differencing technique is not adapted to the statistics of SAR images and non robust to calibration errors [(Vaiyammal, 2018)](https://www.ijert.org/change-detection-on-sar-images)

---

# Identifying change...

** (original) ratio images **

* This is just the images divided....

  * $ratio = \frac{image2}{image1}$

** Improved ratio (IR) **

* Designed to give changed pixels more difference

  * $IR= 1 - \frac{(minI_1(x), min{I_2}(x))}{(maxI_1(x), max{I_2}(x))}$
  * where $minI_1(x), min{I_2}(x)$ is the minimum value between the two images, per pixel


.small[It is a misunderstanding that log ratio outperforms ratio in change detection of SAR images. Source:[Zhuang et al. 2019](https://www.tandfonline.com/doi/pdf/10.1080/22797254.2019.1653226?needAccess=true)
]
---

# Identifying change...

** mean ratio images **

  * $X_m(i,j)= 1 -min( \frac{(u_1(i,j)}{u_2(i,j)},\frac{(u_2(i,j)}{u_1(i,j)})$
  
  
  * where $(u_1(i,j)$ and $(u_2(i,j)$ are the mean values from a neighbourhood in image 1 and 2

** log ratio images **

  * $log ratio = ln\frac{image2}{image1}$

** improved ratio log ratio images **

  * $IRL= ln(1 - \frac{(minI_1(x), min{I_2}(x))}{(maxI_1(x), max{I_2}(x))})$

---

class: inverse, center, middle

## Which is best?

--

## Well, hard to say...

--

## How do we know / how do we test these ?

---

# Testing 

.pull-left[

* In their paper Zhuang et al. (2019) use three existing SAR datasets
  * Berne, Ottawa and FengFeng

* Each of the datasets has a reference map

* Some have been manually created, others aren't specified

* Uses an **ROC curve** (better when closer to upper left)



]

.pull-right[

```{r echo=FALSE, out.width='100%', fig.align='center'}
  knitr::include_graphics('img/zhuang_paper.png')
```

.small[It is a misunderstanding that log ratio outperforms ratio in change detection of SAR images. Source:[Zhuang et al. 2019](https://www.tandfonline.com/doi/pdf/10.1080/22797254.2019.1653226?needAccess=true)
]

]


---

# ROC reminder (Receiver Operating Characteristic Curve)

.pull-left[

* Change the value threshold for the what is labelled as change or not...

* Compare this to the testing data that was manually provided

* Create the True Positive (y axis) and False Positive rates (x axis)

  * True positive = predicted change and is change
  * False positive = predicted change but not change
  * rates are...

$TPR = \frac{number of true positives}{number of true positives + number of false positives}$

$FPR = \frac{number of false positives}{number of false positives + number of true negatvies}$


]

.pull-right[

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/matrix.PNG')
```

.small[Source:[Barsi et al. 2018 Accuracy Dimensions in Remote Sensing](https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-3/61/2018/isprs-archives-XLII-3-61-2018.pdf)]
]


---

class: inverse, center, middle

## What's the problem with these approaches?


## What aren't we making use of?

--

## We aren't using the image collection data

## Just two images in time

---

# Identifying change...through image collections

There are a few ways to do this....

1. Using common statistics and tests we have seen

2. Using specific published methodologies specific to SAR, such as that by Canty et al.(2020), which appears in 

3. Fusing imagery to optical data and then classifying 


---

# Identifying change...statistical tests

In practical 1 we briefly considered a t-test for comparing the difference between Sentinel and Landsat data:

Here we can apply similar logic to test for changes between image **collections**....

$t = \frac{\overline{x1}-\overline{x2}}{\sqrt(S^2(\frac{1}{n1}+\frac{1}{n2}))}$

Where:

* $\overline{x1}$ = mean of pre image collection

* $\overline{x2}$ = mean of post image collection

* ${n1}$ = observations in group 1 (and 2)

* $s2$ = pooled standard deviation (another formula)
---

# Identifying change...statistical tests


.pull-left[
However, a **possible** problem here is that the t-test assume a normal distribution

* In the SAR tutorials on [GEE by Mort Canty](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-1) it suggested that the distribution of SAR data follows a gamma distribution.This means that it is skewed...**however**...

* Andy Field states the normal distribution refers to the difference image not the original images (before and after). In the tutorial by Canty this is done on the ratio image...

* Nevertheless Canty goes on to demonstrate a continuous change detection algorithm for SAR...which can detect dates for changes in the values.
]

.pull-right[

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/mort_canty_gamma.png')
```

.small[Histogram of an single SAR image. Detecting Changes in Sentinel-1 Imagery (Part 1). Source:[Canty 2022](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-1)]

]

---

class: inverse, center, middle

## But do we need that level of complexity ?

--

### Consider an event and a complete image collection (just all the images for the year of the event)

### If change has occured in a pixel we'd expect much higher standard deviation (over time)

###If no change has occured lower standard deviation 

### We can pick (threshold) the pixels that have changed....

---


# Identifying change...image fusion

.pull-left[
Remember from the [video in week 3](https://youtu.be/a4dW5EWbNK4?t=161)

* Decision level fusion
  * Used radar and optical as separate layers (just appending a band to the stack)
  * Then classify 

* Object level fusion
  * Need to make objects from the imagery 
  * Combine optical and SAR (as in decision level)
  * Classify

* Image fusion
  * Pixel values are combined from both optical and SAR
  * **New**  pixel values
  
]


.pull-right[

```{r echo=FALSE, out.width='100%', fig.align='center'}
  knitr::include_graphics('img/pixel_object_fusion.jpg')
```

.small[Overview of the advantages and drawbacks of the most common multispectral-radar SRS image fusion techniques, as well as examples for open-source software to implement them. Source:[Henrike Schulte to Bühne and Nathalie Pettorelli, 2017](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12942)
]

]

---

# Identifying change...image fusion

.pull-left[

* From the figure in past sessions we have seen
  
  * Principal component analysis 
  
  * Object based image analysis
  
  * High pass filtering, same concept with different data here
  
  * Segmentation 
  
* We haven't covered:

  * Intensity hue saturation transformation

  * Wavelet transformation 
 ]
 
 .pull-right[
 
```{r echo=FALSE, out.width='100%', fig.align='center'}
  knitr::include_graphics('img/pixel_object_fusion.jpg')
```

.small[Overview of the advantages and drawbacks of the most common multispectral-radar SRS image fusion techniques, as well as examples for open-source software to implement them. Source:[Henrike Schulte to Bühne and Nathalie Pettorelli, 2017](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12942)
]
 
]
---

# Identifying change...image fusion

.pull-left[

* An example of image fusion from the  Alaska Satellite Facility...

  * Take backscatter from SAR
  
  * Take optical image of same area
  
  * Convert optical from Red, Green Blue (or other mix of bands) to Intensity, Hue and Saturation
  
  * Replace the intensity with SAR data 
  
  * Convert back to RGB
  
Jensen covers this on page 171
]

  
.pull-right[

```{r echo=FALSE, out.width='100%', fig.align='center'}
  knitr::include_graphics('img/fused_image_ASF.jpg')
```

.small[Figure 11: Image fusion result of SAR and optical imagery. Source:[ASF](https://hyp3-docs.asf.alaska.edu/guides/rtc_product_guide/#change-detection-using-rtc-data)
]
 
.small[[ASF has useful SAR guide](https://hyp3-docs.asf.alaska.edu/guides/rtc_product_guide/#change-detection-using-rtc-data)]
]
---

# Identifying change...image fusion


A few things to note...

* IHS and HSV are similar but not the same

* GEE only has the spectral transformation rgsToHsv which is different to IHS. See: [Kamble et al. (2016)](https://www.semanticscholar.org/paper/HSV%2C-IHS-and-PCA-Based-Image-Fusion%3A-A-Review-Kamble-Maisheri/3d36b4232e2b9b8806ff92abd18a6f5ed46918d2). You may need to Google the paper...

* But GEE also has the reverse hsvtoRGB...

* There are online converters for both [RGB to IHS and IHS to RGB](https://www.had2know.org/technology/hsv-rgb-conversion-formula-calculator.html) 
  * You could take these formulas and implement them in GEE
  * Or supplement the value in the GEE function for the SAR data....

---

class: inverse, center, middle

# What should i use

--

## first what are you trying to achieve? 

--

## work backwards

--

## no established way of doing this

--

## whatever works best with reasoning

---

# SAR image 

"multitemporal colour composite SAR image, rice growing areas in the Mekong River delta, Vietnam 1996." 


.pull-left[
"Three SAR images acquired by the ERS satellite during 5 May, 9 June and 14 July in 1996 are assigned to the red, green and blue channels respectively for display. The colourful areas are the rice growing areas, where the landcovers change rapidly during the rice season. The greyish linear features are the more permanent trees lining the canals. The grey patch near the bottom of the image is wetland forest. The two towns appear as bright white spots in this image. An area of depression flooded with water during this season is visible as a dark region."

]

.pull-right[

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/multitemporal colour composite SAR image.png')
```
]

.small[SAR Images. Source:[CRISP](https://crisp.nus.edu.sg/~research/tutorial/sar_int.htm)
]

---

# Me on SAR

> Although SAR images over urban areas provide low quality images due to problems associated with radar imaging in such an environment (i.e. multiple bouncing, layover and shadowing), SAR texture measures can provide valuable information in discerning urban areas (Dell’Acqua et al., 2003; Zhu et al., 2012). Isolated scattering of residential areas and crowded backscatters of inner city high density areas permit classification refinement, thus textural measures such as those descried within the spatial domain can aid identification of alternative urban forms (Zhu et al., 2012). 

>However, the lack of freely available SAR data that temporally coincides with other satellite imagery (e.g. Landsat) frequently precludes extensive use

---
 
# Summary 1

* SAR is an active sensor 

* Records the **amplitude** (backscatter) and **phase** data (location of the wave cycle)

* Consider: 
  * polarization (e.g. VV = surface roughness)
  * Permativity (dielectric constant) - how **reflective** is the property which means **reflective back to the     sensor**. Water usually reflects it off elsewhere
  * Wavelength (or band)

* In GEE we **just have amplitude (backscatter)** not phase

* Data comes in three units
  * power scale (RAW SAR) = statistics
  * amplitude = visualisation 
  * dB scale = seeing differences (default in GEE)

---

# Summary 2

* SAR is very useful as it can **see through clouds** unlike optical sensors (e.g. Landsat) 

* But how can it be useful in our analysis?
  
  * Change between two images (e.g. ratio or log ratios)
  
But this doesn't use the high temporal nature of it!

We can see the variance over time through:
  
  * t-tests
  
  * standard deviation
  
Or we can fuse our SAR data to optical data

  * Principal component analysis
  
  * Object based image analysis
  
  * Intensity fusion

---
