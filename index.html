<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Remotely Sensing Cities and Environments</title>
    <meta charset="utf-8" />
    <meta name="author" content="Andy MacLachlan" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/rladies.css" rel="stylesheet" />
    <link href="libs/remark-css/rladies-fonts.css" rel="stylesheet" />
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <script src="libs/js-cookie/js.cookie.js"></script>
    <script src="libs/peerjs/peerjs.min.js"></script>
    <script src="libs/tiny.toast/toast.min.js"></script>
    <link href="libs/xaringanExtra-broadcast/broadcast.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-broadcast/broadcast.js"></script>
    <script src="libs/freezeframe/freezeframe.min.js"></script>
    <script src="libs/xaringanExtra-freezeframe/freezeframe-init.js"></script>
    <script id="xaringanExtra-freezeframe-options" type="application/json">{"selector":"img[src$=\"gif\"]","trigger":"click","overlay":false,"responsive":true,"warnings":true}</script>
    <script src="libs/fabric/fabric.min.js"></script>
    <link href="libs/xaringanExtra-scribble/scribble.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-scribble/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#FF0000"],"pen_size":3,"eraser_size":30,"palette":[]}) })</script>
    <script src="libs/mark.js/mark.min.js"></script>
    <link href="libs/xaringanExtra-search/search.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-search/search.js"></script>
    <script>window.addEventListener('load', function() { window.xeSearch = new RemarkSearch({"position":"bottom-left","caseSensitive":false,"showIcon":true,"autoSearch":false}) })</script>
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script type="application/json" id="xaringanExtra-editable-docid">{"id":"d12d6c9540074613aaf4ab933a1247be","expires":1}</script>
    <script src="libs/himalaya/himalaya.js"></script>
    <link href="libs/editable/editable.css" rel="stylesheet" />
    <script src="libs/editable/editable.js"></script>
    <script src="libs/xaringanExtra_fit-screen/fit-screen.js"></script>
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-progressBar/progress-bar.js"></script>
    <head>
    <link rel="apple-touch-icon" sizes="180x180" href="assets/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="assets/favicon-16x16.png">
    <link rel="manifest" href="assets/site.webmanifest">
    <link rel="mask-icon" href="assets/safari-pinned-tab.svg" color="#5bbad5">
    </head>
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">






class: center, title-slide, middle

background-image: url("img/CASA_Logo_no_text_trans_17.png")
background-size: cover
background-position: center


&lt;style&gt;
.title-slide .remark-slide-number {
  display: none;
}
&lt;/style&gt;




# Remotely Sensing Cities and Environments

### Lecture 9: Synthetic Aperture Radar (SAR) data

### 4/10/2022 (updated: 29/03/2023)

<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#562457;overflow:visible;position:relative;"><path d="M501.6 4.186c-7.594-5.156-17.41-5.594-25.44-1.063L12.12 267.1C4.184 271.7-.5037 280.3 .0431 289.4c.5469 9.125 6.234 17.16 14.66 20.69l153.3 64.38v113.5c0 8.781 4.797 16.84 12.5 21.06C184.1 511 188 512 191.1 512c4.516 0 9.038-1.281 12.99-3.812l111.2-71.46l98.56 41.4c2.984 1.25 6.141 1.875 9.297 1.875c4.078 0 8.141-1.031 11.78-3.094c6.453-3.625 10.88-10.06 11.95-17.38l64-432C513.1 18.44 509.1 9.373 501.6 4.186zM369.3 119.2l-187.1 208.9L78.23 284.7L369.3 119.2zM215.1 444v-49.36l46.45 19.51L215.1 444zM404.8 421.9l-176.6-74.19l224.6-249.5L404.8 421.9z"/></svg>[a.maclachlan@ucl.ac.uk](mailto:a.maclachlan@ucl.ac.uk)
<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#562457;overflow:visible;position:relative;"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>[andymaclachlan](https://twitter.com/andymaclachlan)
<svg aria-hidden="true" role="img" viewBox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#562457;overflow:visible;position:relative;"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>[andrewmaclachlan](https://github.com/andrewmaclachlan)
<svg aria-hidden="true" role="img" viewBox="0 0 384 512" style="height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#562457;overflow:visible;position:relative;"><path d="M215.7 499.2C267 435 384 279.4 384 192C384 86 298 0 192 0S0 86 0 192c0 87.4 117 243 168.3 307.2c12.3 15.3 35.1 15.3 47.4 0zM192 256c-35.3 0-64-28.7-64-64s28.7-64 64-64s64 28.7 64 64s-28.7 64-64 64z"/></svg>[Centre for Advanced Spatial Analysis, UCL](https://www.ucl.ac.uk/bartlett/casa/)

&lt;a href="https://github.com/andrewmaclachlan" class="github-corner" aria-label="View source on GitHub"&gt;&lt;svg width="80" height="80" viewBox="0 0 250 250" style="fill:#fff; color:#151513; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"&gt;&lt;path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"&gt;&lt;/path&gt;&lt;path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"&gt;&lt;/path&gt;&lt;path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;style&gt;.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}&lt;/style&gt;

---

<style>.xe__progress-bar__container {
  bottom:0;
  opacity: 1;
  position:absolute;
  right:0;
  left: 0;
}
.xe__progress-bar {
  height: 0.25em;
  background-color: #0051BA;
  width: calc(var(--slide-current) / var(--slide-total) * 100%);
}
.remark-visible .xe__progress-bar {
  animation: xe__progress-bar__wipe 200ms forwards;
  animation-timing-function: cubic-bezier(.86,0,.07,1);
}
@keyframes xe__progress-bar__wipe {
  0% { width: calc(var(--slide-previous) / var(--slide-total) * 100%); }
  100% { width: calc(var(--slide-current) / var(--slide-total) * 100%); }
}</style>

# How to use the lectures


- Slides are made with [xaringan](https://slides.yihui.org/xaringan/#1)

- <svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg> In the bottom left there is a search tool which will search all content of presentation

- Control + F will also search 

- Press enter to move to the next result 

- <svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M421.7 220.3l-11.3 11.3-22.6 22.6-205 205c-6.6 6.6-14.8 11.5-23.8 14.1L30.8 511c-8.4 2.5-17.5 .2-23.7-6.1S-1.5 489.7 1 481.2L38.7 353.1c2.6-9 7.5-17.2 14.1-23.8l205-205 22.6-22.6 11.3-11.3 33.9 33.9 62.1 62.1 33.9 33.9zM96 353.9l-9.3 9.3c-.9 .9-1.6 2.1-2 3.4l-25.3 86 86-25.3c1.3-.4 2.5-1.1 3.4-2l9.3-9.3H112c-8.8 0-16-7.2-16-16V353.9zM453.3 19.3l39.4 39.4c25 25 25 65.5 0 90.5l-14.5 14.5-22.6 22.6-11.3 11.3-33.9-33.9-62.1-62.1L314.3 67.7l11.3-11.3 22.6-22.6 14.5-14.5c25-25 65.5-25 90.5 0z"/></svg> In the top right let's you draw on the slides, although these aren't saved.

- Pressing the letter `o` (for overview) will allow you to see an overview of the whole presentation and go to a slide

- Alternatively just typing the slide number e.g. 10 on the website will take you to that slide

- Pressing alt+F will fit the slide to the screen, this is useful if you have resized the window and have another open - side by side. 

<div>
<style type="text/css">.xaringan-extra-logo {
width: 50px;
height: 128px;
z-index: 0;
background-image: url(img/casa_logo.jpg);
background-size: contain;
background-repeat: no-repeat;
position: absolute;
top:1em;right:2em;
}
</style>
<script>(function () {
  let tries = 0
  function addLogo () {
    if (typeof slideshow === 'undefined') {
      tries += 1
      if (tries < 10) {
        setTimeout(addLogo, 100)
      }
    } else {
      document.querySelectorAll('.remark-slide-content:not(.title-slide):not(.inverse):not(.hide_logo)')
        .forEach(function (slide) {
          const logo = document.createElement('div')
          logo.classList = 'xaringan-extra-logo'
          logo.href = null
          slide.appendChild(logo)
        })
    }
  }
  document.addEventListener('DOMContentLoaded', addLogo)
})()</script>
</div>
---
# Lecture outline

.pull-left[

### Part 1: SAR fundamentals 


### Part 2: Practical change detection with SAR 

]

--
.pull-right[
&lt;img src="img/satellite.png" width="100%" /&gt;
.small[Source:[Original from the British Library. Digitally enhanced by rawpixel.](https://www.rawpixel.com/image/571789/solar-generator-vintage-style)
]]

---
class: inverse, center, middle

# Let's recall some of the intro slides....

---

# The two types of sensor

.pull-left[
&lt;img src="img/Passive-and-active-sensors-systems-working-principles-24.png" width="100%" /&gt;
.small[Source: https://www.researchgate.net/figure/Passive-and-active-sensors-systems-working-principles-24_fig2_344464269]
]

.pull-right[
### Active

* Have an energy source for illumination 
* Actively emits electormagentic waves and then waits to receive 
* Such as: Radar, X-ray, LiDAR

### Passive  

* Use energy that is available
* Don't emit anything 
* Usually detecting **reflected** energy from the sun
* Energy is in electromagnetic waves...
* Such as: Human eye, camera, satellite sensor

Sensors can be mounted on any platform. 
]


---
class: inverse, center, middle

# Now some of the lecture 4 slides...and a few extras

---

# SAR floods

**Sensor**

* Sentinel-1 SAR 

ENSO phases but this is from Australian La Niña 2022
  * trade winds from south america intensity
  * draw up cool deep waters and increase thermocline 
  * temp difference increases, walker circulation intensifies - feedback loop
  * more cloud + more rain + cyclones in West Pacific 

&lt;img src="img/SAR1.jpg" width="35%" style="display: block; margin: auto;" /&gt;

.small[Eastern Australia Floods. Source:[brockmann-consult](https://www.brockmann-consult.de/eastern-australia-floods/)
]


---

# SAR background

Synthetic Aperture Radar:
  * Active sensors
  * Have surface texture data
  * See through weather and clouds 
  * Different wavelengths - different applications
  
&lt;img src="img/SAR_bands.png" width="50%" style="display: block; margin: auto;" /&gt;

.small[What is Synthetic Aperture Radar?. Source:[NASA Earth Data](https://earthdata.nasa.gov/learn/backgrounders/what-is-sar)
]


---
# SAR background
.pull-left[

1. Emit an electromagnetic signal (speed of light)
1. Record the amount of signal that bounces back = "backscatter"

But...the Radar is moving...

1. Moves forward (in the azimuth) - longer antenna has a narrower beam and high resolution
1. Sweeps the footprint (swath)

Images...

1. Pixels in swath are imaged many times
1. **means** the distance will change and we know exactly how much by phase...
1. We combine these images to make **"synthetic" aperture**
]

.pull-right[

&lt;img src="img/Radar_in_motion_diagram.jpeg" width="100%" style="display: block; margin: auto;" /&gt;

.small[Get to know SAR. Source: [NASA](https://nisar.jpl.nasa.gov/mission/get-to-know-sar/overview/)
]
]

---
#SAR terms

.pull-left[
* Photography **aperture** = lets more light in to change focus
&lt;img src="img/aperture-rules.jpg" width="100%" style="display: block; margin: auto;" /&gt;
.small[What is Aperture? Source:[City Academy](https://www.city-academy.com/news/what-is-aperture-in-photography/)
]
]
.pull-right[

* RADAR aperture = the antenna 
&lt;img src="img/seasat_landing_image-300x300.png" width="100%" style="display: block; margin: auto;" /&gt;
.small[What is Synthetic Aperture Radar? Source:[NASA Earth Data](https://earthdata.nasa.gov/learn/backgrounders/what-is-sar)
]
]

---

class: inverse, center, middle

## Longer antenna = narrower beam and a higher resolution


## BUT we can't have a long antenna in space..

## "synthetic" aperture = "synthesize a long antenna by combining signals, or echoes, received by the radar as it moves along a flight track"


.small[[Source: NASA get to know SAR](https://nisar.jpl.nasa.gov/mission/get-to-know-sar/overview/)
]

---

# SAR polarization

.pull-left[

  * Also different ploarizations:
    * orientation of the plane in which EMR waves transmitted..
    * "direction of travel of an electromagnetic wave vector’s tip: vertical (up and down), horizontal (left to right), or circular (rotating in a constant plane left or right)."
    
* Single = 1 horizontal (or vertical)  

* Dual = transmits and receives both horizontal and vertical

* HH = emitted in horizontal (H) and received in horizontal (H)
    
]

.pull-right[

&lt;img src="img/polarisation.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[Polarization. Source:[Wetland Monitoring and Mapping Using Synthetic Aperture Radar](https://www.intechopen.com/chapters/63701)
]

]
 
---
  
# SAR polarization

Different surfaces respond differently to the polarizations

* Rough scattering (e.g. bare earth) = most sensitive to VV
* Volume scattering (e.g. leaves) = cross, VH or HV
* Double bounce (e.g. trees / buildings) = most sensitive to HH.

&lt;img src="img/SARPolarization.jpg" width="100%" style="display: block; margin: auto;" /&gt;

.small[What is Synthetic Aperture Radar?. Source:[NASA Earth Data](https://earthdata.nasa.gov/learn/backgrounders/what-is-sar)
]

---

# SAR background 

Scattering can change based on wavelength 

Further penetration then the volume scattering will change

&lt;img src="img/SARtree_figure2.jpg" width="100%" style="display: block; margin: auto;" /&gt;

.small[What is Synthetic Aperture Radar?. Source:[NASA Earth Data](https://earthdata.nasa.gov/learn/backgrounders/what-is-sar)
]


---

# SAR background 

.pull-left[

* Wavelength of SAR can change application 

&lt;img src="img/SAR_bands_NASA.png" width="100%" style="display: block; margin: auto;" /&gt;
]

.pull-right[

* Remember this is on the electromagnetic spectrum (EMR)

&lt;img src="img/SAR_bands_EMR.jpg" width="100%" style="display: block; margin: auto;" /&gt;
]
.small[What is Synthetic Aperture Radar?. Source:[NASA Earth Data](https://earthdata.nasa.gov/learn/backgrounders/what-is-sar)
]

---

# Amplitude (backscatter) and phase


.pull-left[
A SAR signal has both **amplitude** (backscatter) and **phase** data

** Backscatter (amplitude)**

* Polarization

  * VV = surface roughness
  * VH = volume of surface (e.g. vegetation has a complex volume and can change the polarization)

* Permativity (dielectric constant) - how **reflective** is the property which means **reflective back to the sensor**. Water usually reflects it off elsewhere

* The return **value**, also remember the band (wavelength)
]

.pull-right[

&lt;img src="img/amplitude_example.png" width="100%" style="display: block; margin: auto;" /&gt;
* Wind makes the water move and reflect back to the sensor (under VV)

.small[NASA Data Made Easy: Part 2- Introduction to SAR. Source:[NASA Earth Data](https://www.youtube.com/watch?v=Zfn7P395O40)
]
]

---

# Amplitude (backscatter) and phase

.pull-left[
A SAR signal has both **amplitude** (backscatter) and **phase** data
  
**Phase**

  * Location of wave on the cycle when it comes back to the sensor

]

.pull-right[

&lt;img src="img/phas_shift.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[InSAR. Source:[Pascal Castellazzi](https://www.researchgate.net/figure/Principle-of-the-InSAR-techniques-the-phase-difference-observed-by-comparing-two-SAR_fig1_342916517)
]
]

---
# InSAR

InSAR is mapping for ground movement detected through this **phase shift**

Movement is shown through an **interferogram**

&lt;img src="img/phase_shift.jpg" width="70%" style="display: block; margin: auto;" /&gt;

.small[InSAR. Source:[GeoScience Australia](https://www.ga.gov.au/scientific-topics/positioning-navigation/geodesy/geodetic-techniques/interferometric-synthetic-aperture-radar)
]

---

# Differential Interferometric Synthetic Aperture Radar (DInSAR)

The field of this work is called **interferometry**

Phase shift might come from topography (hills etc) if this is the case (it might not be) then we can remove the effect and this is termed **Differential Interferometry (DInSAR)** 

Typical workflow will include considering the elevation and 


---

# InSAR or DInSAR

* SAR = active sensor, see through clouds, records energy reflected back 

* InSAR = used for DEMs, converting phase different to relative height

* DInSAR = changes between two images in time. Looking at movement of land (uplift or sinking) with topography removed (using a DEM)

---

# SAR data processing


.pull-left[
Each pixel is a real/imaginary number know as I and  Q, meaning an in-phase(I) and quadrature pair (Q), this comes from ** electrical engineering**

To create a visible image (amplitude) it is **detected** = the Ground Range Detection produce in GEE:

  * square root of the sum of squared from the I and Q values in the Single Look Complex (SLC) product
  * This makes intensity (backscatter) 

Values can be stored in power, amplitude or dB....
]

.pull-right[
&lt;img src="img/product_tree.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[The Tree of Processing Options. Source:[ICEYE](https://iceye-ltd.github.io/product-documentation/5.0/productFormats/introduction/)
]

]


---

class: inverse, center, middle

## In GEE

## Only **amplitude (backscatter)** data is available..

## To use phase data we need to use SNAP (not considered here) 

---

# GEE SAR data

* Notice that it is logged - why?
  * Show the full range of values
* To get non logged data - why?
  * Undertake calculations 


```r
ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT')
```

&lt;img src="img/SAR_GEE.png" width="70%" style="display: block; margin: auto;" /&gt;

---

# SAR data values

Sentinel-1 Radiometric Terrain Corrected (RTC) data is typically provided on a **power** scale...

** power scale **, **RAW data**

* These values are low and close to 0
  * means that bright areas are skewed (you can't see any difference)
  * good for statistics / analysis, poor for visualisation 

** amplitude scale **

* square root of the power scale values
  * brighter darker pixels and darken bright pixels - narrows range
  * good for visualisation + log changes (see later slides)

** dB scale **, **in GEE**

* multiplying 10 times the Log10 of the power scale values
  * good for identifying differences in dark pixels (e.g. water)
  * not great for visualistion can be "washed out"
  * not useful for statistical analysis (log scale)

.small[Introduction to SAR. Source:[Alaska Satellite Facility](https://hyp3-docs.asf.alaska.edu/guides/introduction_to_sar/#introduction-to-sar)
]


---

# Examples 

See [the scale conversion tool by Heidi Kristenson, ASF](https://storymaps.arcgis.com/stories/73b6af082e1f44bca8a0c5fb6bf09f37)

The [power of SAR seeing through clouds](https://storymaps.arcgis.com/stories/2ead3222d2294d1fae1d11d3f98d7c35)

---

# GEE SAR data...a closer look

&lt;img src="img/SAR_GEE.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# Questions to help us decide

We need to consider...

**What are we trying to detect, roughness or volume of a material...**

* Polarization

  * VV = surface roughness
  * VH = volume of surface (e.g. vegetation has a complex volume and can change the polarization)

**What are we trying to do...**

* Type of data

  * power scale = analysis
  * amplitude scale = visualisation 
  * dB scale = dark pixel differences

**what part of earth are we looking at...**

  * lot's of water?

---

class: inverse, center, middle

## How to we identify change then

--

## At the moment there seems to be no consistent approach to this question...

---

# Identifying change...

As we now know enough about SAR data...how do we establish changes between time?

** subtract images **
* Sometimes with optical data we can subtract images to determine differences

  * This is not a good idea with SAR data

&gt; So difference pixels in bright areas will have a higher variance than difference pixels in darker areas [(Mort Canty)](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-2)

&gt; image differencing technique is not adapted to the statistics of SAR images and non robust to calibration errors [(Vaiyammal, 2018)](https://www.ijert.org/change-detection-on-sar-images)

---

# Identifying change...

** (original) ratio images **

* This is just the images divided....

  * `\(ratio = \frac{image2}{image1}\)`

** Improved ratio (IR) **

* Designed to give changed pixels more difference

  * `\(IR= 1 - \frac{(minI_1(x), min{I_2}(x))}{(maxI_1(x), max{I_2}(x))}\)`
  * where `\(minI_1(x), min{I_2}(x)\)` is the minimum value between the two images, per pixel


.small[It is a misunderstanding that log ratio outperforms ratio in change detection of SAR images. Source:[Zhuang et al. 2019](https://www.tandfonline.com/doi/pdf/10.1080/22797254.2019.1653226?needAccess=true)
]
---

# Identifying change...

** mean ratio images **

  * `\(X_m(i,j)= 1 -min( \frac{(u_1(i,j)}{u_2(i,j)},\frac{(u_2(i,j)}{u_1(i,j)})\)`
  
  
  * where `\((u_1(i,j)\)` and `\((u_2(i,j)\)` are the mean values from a neighbourhood in image 1 and 2

** log ratio images **

  * `\(log ratio = ln\frac{image2}{image1}\)`

** improved ratio log ratio images **

  * `\(IRL= ln(1 - \frac{(minI_1(x), min{I_2}(x))}{(maxI_1(x), max{I_2}(x))})\)`

---

class: inverse, center, middle

## Which is best?

--

## Well, hard to say...

--

## How do we know / how do we test these ?

---

# Testing 

.pull-left[

* In their paper Zhuang et al. (2019) use three existing SAR datasets
  * Berne, Ottawa and FengFeng

* Each of the datasets has a reference map

* Some have been manually created, others aren't specified

* Uses an **ROC curve** (better when closer to upper left)



]

.pull-right[

&lt;img src="img/zhuang_paper.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[It is a misunderstanding that log ratio outperforms ratio in change detection of SAR images. Source:[Zhuang et al. 2019](https://www.tandfonline.com/doi/pdf/10.1080/22797254.2019.1653226?needAccess=true)
]

]


---

# ROC reminder (Receiver Operating Characteristic Curve)

.pull-left[

* Change the value threshold for the what is labelled as change or not...

* Compare this to the testing data that was manually provided

* Create the True Positive (y axis) and False Positive rates (x axis)

  * True positive = predicted change and is change
  * False positive = predicted change but not change
  * rates are...

`\(TPR = \frac{number of true positives}{number of true positives + number of false positives}\)`

`\(FPR = \frac{number of false positives}{number of false positives + number of true negatvies}\)`


]

.pull-right[

&lt;img src="img/matrix.PNG" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[Barsi et al. 2018 Accuracy Dimensions in Remote Sensing](https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-3/61/2018/isprs-archives-XLII-3-61-2018.pdf)]
]


---

class: inverse, center, middle

## What's the problem with these approaches?


## What aren't we making use of?

--

## We aren't using the image collection data

## Just two images in time

---

# Identifying change...through image collections

There are a few ways to do this....

1. Using common statistics and tests we have seen

2. Using specific published methodologies specific to SAR, such as that by Canty et al.(2020), which appears in 

3. Fusing imagery to optical data and then classifying 


---

# Identifying change...statistical tests

In practical 1 we briefly considered a t-test for comparing the difference between Sentinel and Landsat data:

Here we can apply similar logic to test for changes between image **collections**....

`\(t = \frac{\overline{x1}-\overline{x2}}{\sqrt(S^2(\frac{1}{n1}+\frac{1}{n2}))}\)`

Where:

* `\(\overline{x1}\)` = mean of pre image collection

* `\(\overline{x2}\)` = mean of post image collection

* `\({n1}\)` = observations in group 1 (and 2)

* `\(s2\)` = pooled standard deviation (another formula)
---

# Identifying change...statistical tests


.pull-left[
However, a **possible** problem here is that the t-test assume a normal distribution

* In the SAR tutorials on [GEE by Mort Canty](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-1) it suggested that the distribution of SAR data follows a gamma distribution.This means that it is skewed...**however**...

* Andy Field states the normal distribution refers to the difference image not the original images (before and after). In the tutorial by Canty this is done on the ratio image...

* Nevertheless Canty goes on to demonstrate a continuous change detection algorithm for SAR...which can detect dates for changes in the values.
]

.pull-right[

&lt;img src="img/mort_canty_gamma.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[Histogram of an single SAR image. Detecting Changes in Sentinel-1 Imagery (Part 1). Source:[Canty 2022](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-1)]

]

---

class: inverse, center, middle

## But do we need that level of complexity ?

--

### Consider an event and a complete image collection (just all the images for the year of the event)

### If change has occured in a pixel we'd expect much higher standard deviation (over time)

###If no change has occured lower standard deviation 

### We can pick (threshold) the pixels that have changed....

---


# Identifying change...image fusion

.pull-left[
Remember from the [video in week 3](https://youtu.be/a4dW5EWbNK4?t=161)

* Decision level fusion
  * Used radar and optical as separate layers (just appending a band to the stack)
  * Then classify 

* Object level fusion
  * Need to make objects from the imagery 
  * Combine optical and SAR (as in decision level)
  * Classify

* Image fusion
  * Pixel values are combined from both optical and SAR
  * **New**  pixel values
  
]


.pull-right[

&lt;img src="img/pixel_object_fusion.jpg" width="100%" style="display: block; margin: auto;" /&gt;

.small[Overview of the advantages and drawbacks of the most common multispectral-radar SRS image fusion techniques, as well as examples for open-source software to implement them. Source:[Henrike Schulte to Bühne and Nathalie Pettorelli, 2017](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12942)
]

]

---

# Identifying change...image fusion

.pull-left[

* From the figure in past sessions we have seen
  
  * Principal component analysis 
  
  * Object based image analysis
  
  * High pass filtering, same concept with different data here
  
  * Segmentation 
  
* We haven't covered:

  * Intensity hue saturation transformation

  * Wavelet transformation 
 ]
 
 .pull-right[
 
&lt;img src="img/pixel_object_fusion.jpg" width="100%" style="display: block; margin: auto;" /&gt;

.small[Overview of the advantages and drawbacks of the most common multispectral-radar SRS image fusion techniques, as well as examples for open-source software to implement them. Source:[Henrike Schulte to Bühne and Nathalie Pettorelli, 2017](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12942)
]
 
]
---

# Identifying change...image fusion

.pull-left[

* An example of image fusion from the  Alaska Satellite Facility...

  * Take backscatter from SAR
  
  * Take optical image of same area
  
  * Convert optical from Red, Green Blue (or other mix of bands) to Intensity, Hue and Saturation
  
  * Replace the intensity with SAR data 
  
  * Convert back to RGB
  
Jensen covers this on page 171
]

  
.pull-right[

&lt;img src="img/fused_image_ASF.jpg" width="100%" style="display: block; margin: auto;" /&gt;

.small[Figure 11: Image fusion result of SAR and optical imagery. Source:[ASF](https://hyp3-docs.asf.alaska.edu/guides/rtc_product_guide/#change-detection-using-rtc-data)
]
 
.small[[ASF has useful SAR guide](https://hyp3-docs.asf.alaska.edu/guides/rtc_product_guide/#change-detection-using-rtc-data)]
]
---

# Identifying change...image fusion


A few things to note...

* IHS and HSV are similar but not the same

* GEE only has the spectral transformation rgsToHsv which is different to IHS. See: [Kamble et al. (2016)](https://www.semanticscholar.org/paper/HSV%2C-IHS-and-PCA-Based-Image-Fusion%3A-A-Review-Kamble-Maisheri/3d36b4232e2b9b8806ff92abd18a6f5ed46918d2). You may need to Google the paper...

* But GEE also has the reverse hsvtoRGB...

* There are online converters for both [RGB to IHS and IHS to RGB](https://www.had2know.org/technology/hsv-rgb-conversion-formula-calculator.html) 
  * You could take these formulas and implement them in GEE
  * Or supplement the value in the GEE function for the SAR data....

---

class: inverse, center, middle

# What should i use

--

## first what are you trying to achieve? 

--

## work backwards

--

## no established way of doing this

--

## whatever works best with reasoning

---

# SAR image 

"multitemporal colour composite SAR image, rice growing areas in the Mekong River delta, Vietnam 1996." 


.pull-left[
"Three SAR images acquired by the ERS satellite during 5 May, 9 June and 14 July in 1996 are assigned to the red, green and blue channels respectively for display. The colourful areas are the rice growing areas, where the landcovers change rapidly during the rice season. The greyish linear features are the more permanent trees lining the canals. The grey patch near the bottom of the image is wetland forest. The two towns appear as bright white spots in this image. An area of depression flooded with water during this season is visible as a dark region."

]

.pull-right[

&lt;img src="img/multitemporal colour composite SAR image.png" width="100%" style="display: block; margin: auto;" /&gt;
]

.small[SAR Images. Source:[CRISP](https://crisp.nus.edu.sg/~research/tutorial/sar_int.htm)
]

---

# Me on SAR

&gt; Although SAR images over urban areas provide low quality images due to problems associated with radar imaging in such an environment (i.e. multiple bouncing, layover and shadowing), SAR texture measures can provide valuable information in discerning urban areas (Dell’Acqua et al., 2003; Zhu et al., 2012). Isolated scattering of residential areas and crowded backscatters of inner city high density areas permit classification refinement, thus textural measures such as those descried within the spatial domain can aid identification of alternative urban forms (Zhu et al., 2012). 

&gt;However, the lack of freely available SAR data that temporally coincides with other satellite imagery (e.g. Landsat) frequently precludes extensive use

---
 
# Summary 1

* SAR is an active sensor 

* Records the **amplitude** (backscatter) and **phase** data (location of the wave cycle)

* Consider: 
  * polarization (e.g. VV = surface roughness)
  * Permativity (dielectric constant) - how **reflective** is the property which means **reflective back to the     sensor**. Water usually reflects it off elsewhere
  * Wavelength (or band)

* In GEE we **just have amplitude (backscatter)** not phase

* Data comes in three units
  * power scale (RAW SAR) = statistics
  * amplitude = visualisation 
  * dB scale = seeing differences (default in GEE)

---

# Summary 2

* SAR is very useful as it can **see through clouds** unlike optical sensors (e.g. Landsat) 

* But how can it be useful in our analysis?
  
  * Change between two images (e.g. ratio or log ratios)
  
But this doesn't use the high temporal nature of it!

We can see the variance over time through:
  
  * t-tests
  
  * standard deviation
  
Or we can fuse our SAR data to optical data

  * Principal component analysis
  
  * Object based image analysis
  
  * Intensity fusion

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
